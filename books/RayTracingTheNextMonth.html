<meta charset="utf-8">
<!-- Markdeep: https://casual-effects.com/markdeep/ -->



                                   **Ray Tracing: The Next Month**
                                          Trevor David Black
                                                <br>
                                     Version 0.1.0-wip, 2020-XXX-XX
                                                <br>
                        Copyright 2020 Trevor David Black. All rights reserved.



Overview
====================================================================================================

Okay, so you've written a ray tracer.

In _In One Weekend_ you learned about the ray tracing paradigm and managed to render a collection of
spheres with varying material properties. You moved on to _The Next Month_ and vastly increased your
ray tracing knowledge; combining lights, quads, spheres, and volumes to produce vastly more
complicated scenes. With as much of an increase in visual fidelity that came from book one to book
two, the hope of this book is:

To set the reader on the path of writing commercially viable path tracing software.

Something needs to be made explicit: This book, _The Next Month_, is chronilogically the fourth book
in the _Ray Tracing_ series, being written after _In One Weekend_, _The Next Week_, and
_The Rest of Your Life_. However, it is the intention of the author that this book will be placed
logically as the third book in the series, falling between _The Next Week_ and
_The Rest of Your Life_. You are expected to have completed _In One Weekend_ and _The Next Week_.

This book is written in the hope that the reader will go through the chapters in order. If you find
that a specific chapter isn't particularly helpful to your interests, then you may be able to skip
it. This book is structured in blocks of chapters, where any given block will have very little
reliance on content found in a different block. You can comfortably skip over any block of chapters
without too much difficulty. The chapters would be split up this way:

Block I (Prologue)
- Multithreading
Block II
- SRT Affine Transformations
- Triangles
- Meshes
- Obj Loading
- Cleanup
Block III
- BRDF
- Diffuse Materials
- Glossy Materials
- BSSRDF
Block IV (Intermission)
- Light Paths and AOVs
Block V
- Make a Video
- Keyframes
- Shadow Rays
Block VI (Epilogue)
- Environment Map

There is a lot of ground to cover in this book, most of which still needs writing, but without any
further ado...

Happy tracing.

Of course, any bugs/complaints/feedback are very much welcome.


Multithreading
====================================================================================================

By now, you've run into the following experience:

You start up a render of a complicated scene at high sample count or high pixel dimension, or
_both_. Your computer's fan spins up to a loud degree and then you run off and get a coffee.

By the end of _The Next Week_ you were creating images where if you wanted them to have low sampling
noise you were needing to run them for 5, 10, maybe even 20 minutes. _Well_ the bad news is that
your renders are still going to take a long time. _But_, the good news is that we'll be rendering
much more complicated scenes. It's not my intention to spend too much of this time talking about
runtime performance and teaching you performance optimizations to make your renderers faster. When
you decided you wanted to ray trace you were sacrificing rendering time for visual fidelity, it just
means that you'll have a _lot_ of coffees to go grab. And, more to the point, the number and
sophistication of useful ray tracing optimizations really deserve their own book (or several).

In _The Next Week_ you learned about an extremely effective optimization in the bounding volume
hierarchy, or BVH, as it's commonly abbreviated. In the scenes that can take advantage of BVH
acceleration, the optimization changes your scene traversal from being _linear_ with scene
complexity, to being _logarithmic_ with scene complexity. This is a massive speed-up, for a scene
with 1,000,000 objects, the linear algorithm will take order 1,000,000 checks, whereas the
logarithmic algorithm will take order 1,000 checks.

However, you really won't see perfectly logarithmic complexity with any useful scene in practice,
and you probably found a couple of renders where the addition of the BVH actually **increased**
runtime. This leads us naturally to multithreading. Up to now, you've only taken advantage of a
single operating system thread, but if you've been running your renders on a computer built in the
last 15 years, you haven't been taking advantage of your computers full hardware. Any modern machine
is going to have a collection of distinct hardware cores that can each run an independent program.
These distinct hardware blocks frequently operate independently and run different programs: core 1
might be running your browser, core 2 might be running your text editor, core 3 might be running
your music player. But this only partly the truth, these hardware cores are frequently running the
same program, cores 1 through N of your machine are all probably managing your browser. For any
given program in c++ we can spin up new software threads that get mapped to a hardware core. These
hardware cores operate (largely) independently and can be made to do more work. We can spin up $N$
threads, where $N = number of hardware threads$ to speed up our render by something close to $N$.

You may note that this isn't a speedup from linear to logarithmic, but rather a speedup from linear
to linear. That may not seem as powerful, but the results can still be incredible. The author's
laptop has 6 hardware cores with 12 logical cores (more on that in a second). For a render that
takes 15 minutes while running in a single thread, the hope is that the render would take $15/6$ or
$2.5$ minutes in 6 threads.

As stated, I don't want to spend too long in this book talking about runtime performance, but some
of the additions required to get multithreading working are required for some of the more
interesting developments later on in the book. And, it's a pretty big performance win for not a lot
of code.

As for 6 hardware cores-12 logical cores statement earlier, many desktop CPUs can run multiple
software threads within one hardware thread. Intel refers to this as Hyperthreading, and AMD refers
to this as Simultaneous Multithreading (or SMT). The newer your computer, the more likely it can do
this. The underlying math part of a hardware core is shared between these 2 (frequently 2,
sometimes more) software threads, and it switches whenever the CPU needs to read something from
memory (10,000ft summary). It isn't accurate to say that a hardware core running 2 software threads
is going to see a 2x increase in the number of work it can do. But it's reasonable to expect a 1.3x
increase, so it's reasonable to expect something around $6 * 1.3 = 7.8$ increase over
singlethreaded rendering. But only if we are able to take advantage of all 12 logical cores.

Averaging N Photos
----------------------------------------------------------------------------------------------------

We can start with an instructive, if limiting, exercise. Don't worry about implementing any of the
changes seen in this section, they're meant for instruction, not implementation.

If we focus our attention to the rendering loop we'll notice a few things:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
    for (int j = image_height-1; j >= 0; --j) {
        std::cerr << "\rScanlines remaining: " << j << ' ' << std::flush;
        for (int i = 0; i < image_width; ++i) {
            vec3 color;
            for (int s = 0; s < samples_per_pixel; ++s) {
                auto u = (i + random_double()) / image_width;
                auto v = (j + random_double()) / image_height;
                ray r = cam.get_ray(u, v);
                color += ray_color(r, background, world, max_depth);
            }
            color.write_color(std::cout, samples_per_pixel);
        }
    }
 
    std::cerr << "\nDone.\n";
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    [Listing [render-loop]: <kbd>[main.cc]</kbd> The rendering loop the program runs over]

This is all of the work of our program, there are a few things to set up like camera configuration
and scene build up, but 99.9% of our runtime exists within this code block. A few things of value:

1. It's a triply-nest loop, meaning it has 3 layers of for loops
2. We would expect that `ray_color` is where the majority of our runtime comes from
3. The number of `ray_color` invocations appears to scale with
`image_height`*`image_width`*`samples_per_pixel`

If you've played around with image dimension or pixel sampling, you'll have noticed that the program
time of your render will have scaled linearly with the number of pixels or their sample count. If we
can figure out how to split up these `ray_color` invocations into different programs or different
logical threads, then we would expect a linear speedup. This is multithreading.

The simplest way we can accomplish this is by splitting up our render. We can take our render and
split it in half: We render our scene twice, but half the sample count in each program, and then
take the average of both. This should produce an image with the image dimensions and samples per
pixel of the original image, but will have run in two different logical threads, whereby taking
better advantage of our underlying hardware.

We'll start this example at the end of _The Next Week_. Taking the final scene of that book, we'll
render at 32 samples per pixel as our baseline:

  <div class="render">

  ![The Next Week final scene in 32 spp](../images/img.book2-final_32spp.png)

  </div>

With that render completed, we will take advantage of our operating system's threads by spinning up
two instances of the render. We'll start up one render at 16 samples per pixel, and then start up
another render at 16 samples per pixel. If our operating system has threads available, it should
run both of our renders seperately, and we should see both take about half as long as the 32 samples
per pixel run from earlier.

The first 16spp render: 
  <div class="render">

  ![The Next Week final scene in 16 spp (first render)](../images/img.book2-final_16spp.png)

  </div>

And here is the second 16spp render:
  <div class="render">

  ![The Next Week final scene in 16 spp (second render)](../images/img.book2-final_16spp.png)

  </div>

Both of our outputted images were created in the ppm file format. We need to take the average of
these two photos to approximate the 32 sample per pixel image. We can either write a ppm loader
or take advantage of existing tools. Any photo editing software like Photoshop, GIMP, Krita, etc.
will be able to combine two photos. You can write a ppm loader if you're interested, but I will
just be combining these in a photo editor. If you're unsure of how to do this, don't worry, you
won't need to explicitly combine the images to follow along.

When we use a photo editor to combine our two 16spp images we get:
  <div class="render">

  ![The Next Week final scene in 16 spp (second render)](../images/img.book2-final_16spp.png)

  </div>

Which, does not look like 32 samples per pixel, it doesn't even look better than 16 samples per
pixel. What is going on here?

Well, all three images: the first 16spp render, the second 16spp render, and the combined 32spp
render are all perfectly identical. They are pixel perfect copies of one another. The two 16spp
renders are themselves identical, and the average of the two gives us more of the same.

This leads us to wonder why the original 16 samples per pixel are identical. When we wrote the
rendering loop, we made sure to randomize the location of rays to remove aliasing:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
    for (int j = image_height-1; j >= 0; --j) {
        std::cerr << "\rScanlines remaining: " << j << ' ' << std::flush;
        for (int i = 0; i < image_width; ++i) {
            vec3 color;
            for (int s = 0; s < samples_per_pixel; ++s) {
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++ highlight
                auto u = (i + random_double()) / image_width;
                auto v = (j + random_double()) / image_height;
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
                ray r = cam.get_ray(u, v);
                color += ray_color(r, background, world, max_depth);
            }
            color.write_color(std::cout, samples_per_pixel);
        }
    }
 
    std::cerr << "\nDone.\n";
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    [Listing [render-loop-randomness]: <kbd>[main.cc]</kbd> Sources of random in the rendering loop]

The `u` and `v` coordinates of the ray within the image should be different between different
renders. If the `u` and `v` values for one ray are the same as for another ray, we might be
sampling the same part of the scene and we wouldn't be getting any new information. Let's dig into
our `random_double` function:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
    inline double random_double() {
        return rand() / (RAND_MAX + 1.0);
    }
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    [Listing [random_double]: <kbd>[rtweekend.h]</kbd> The random_double function]

Okay, `random_double` calls into `rand`. `rand` is a c++ function found in `<cstlib>` that returns
pseudo-random number betwen `0` and `RAND_MAX` using a seed. A couple of important facts in that
sentence. The first thing is that `rand` is not a truly random number generator, calls to `rand` are
not truly random, rather, calls to `rand` follow a deterministic pattern. This is the reason that
different calls to the program produce identical results, any sources of random will be the same for
both runs. So, even though we were hoping to get different data about our scene, we receive the same
data. So much for random. It's not all problematic, though. What we can do is change what the
pattern is.

For any given seed entered into a pseudo-random number generator, that generator will always produce
the same string of "random" numbers. If we change the seed for that random generator it will produce
a different string of "random" numbers. For the function `rand` there exists the function `srand`
that will change the seed that `rand` calls into. All we need to do is call `srand` on `main`:

    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
    #include <iostream>
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++ highlight
    #include <ctime>
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
    ...
    int main() {
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++ highlight
        srand(time(NULL));
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ C++
        const int image_width = 768;
        const int image_height = 768;
        const auto aspect_ratio = double(image_width) / image_height;

        hittable_list world;

        int samples_per_pixel = 16;
        int max_depth = 50;

        vec3 lookfrom;
        vec3 lookat;
        vec3 vup(0,1,0);
        auto vfov = 40.0;
        auto aperture = 0.0;
        auto dist_to_focus = 10.0;
        vec3 background(0,0,0);

        ...
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    [Listing [changing-rand-seed]: <kbd>[main.cc]</kbd> Changing the seed for rand on program start]

The argument to `srand` just tells `srand` to set the seed to the current time. Now when we call
`rand` we'll get different random numbers, and we should receive different images from our renders.
Indeed, we do:

  <div class="render">

  ![The Next Week final scene in 16 spp (first seed)](../images/img.book2-final_new-seed-1_16spp.png)

  </div>

  <div class="render">

  ![The Next Week final scene in 16 spp (second seed)](../images/img.book2-final_new-seed-2_16spp.png)

  </div>


<!-- Fix perlin bug in final scene --> 

<!-- Consider rerendering final scene in 32,64 spp --> 

<!-- "Show off new averaging"--> 

<!-- "Explain why it fails for random parts of our scene"--> 

<!-- "Say that we can map random parts of the scene to a constant seed"-->



The lesson here is that if you have multiple runs of a render you need to be cognizant of where your
random numbers are coming from, and where they are going.

<!-- "But don't do that, do this instead, leading to next chapter"--> 





<!-- Markdeep: https://casual-effects.com/markdeep/ -->
<link rel='stylesheet' href='../style/book.css'>
<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<script src="markdeep.min.js"></script>
<script src="https://casual-effects.com/markdeep/latest/markdeep.min.js"></script>
<script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
